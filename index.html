<!DOCTYPE HTML>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="author" content="Chaonan Ji">
  <meta name="description" content="Chaonan Ji's Homepage">
  <meta name="keywords" content="Chaonan Ji,吉朝南,homepage,主页,Master,computer vision,Tsinghua,3D vision, Neural rendering, Digital avatar>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Chaonan Ji(吉朝南)'s Homepage</title>
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>JChaonan Ji(吉朝南)</name>
              </p>
              <p style="text-align:center">
                Email: 18801313326[at]163.com &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;<a href="https://github.com/jcn16">Github</a>
              </p>
              <p>
		I am the second year of postgraduate student from Tsinghua University, pursuing Master Degree. I have been doing researches at BBNC with Professor <a href="https://liuyebin.com/">Yebin Liu in the field of inverse rendering and 3D vision. My research interest typically lies in **3D Vision, Neural Rendering and Animatable Avatar**. 
              </p>
            </td>
            <td style="padding:15% 7% 7% 7%;width:40%;max-width:40%">
              <a href="images/sunjingxiang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/sunjingxiang.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        
<!--           <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
  <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
              <source src='images/teaser_ide_3d.mp4'>
  </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>IDE-3D: Interactive Disentangled Editing for High-Resolution 3D-aware Portrait Synthesis</papertitle>
              <br>
              <strong>Jingxiang Sun</strong>, Xuan Wang, Yichun Shi, Lizhen Wang, Jue Wang, Yebin Liu
              <br>
              <em>Arxiv </em> 2022, 
              <br>
              <a href="https://mrtornado24.github.io/IDE-3D/">[Project]</a>
              <a href="https://link.springer.com/article/10.1007/s11263-022-01596-7">[PDF]</a>
              <a href="images/imocap.txt">[BibTeX]</a>
              <br>
              <p> we propose a high-resolution 3D-aware generative model that not only enables local control of the facial shape and texture, but also supports real-time, interactive editing. </p>
          </td>
      </tr>
 -->
        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
                    <img src='images/cvpr22_fenerf.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>FENeRF: Face Editing in Neural Radiance Fields</papertitle>
                <br>
                <strong>Jingxiang Sun</strong>, Xuan Wang, Yong Zhang, Xiaoyu Li, Qi Zhang, Yebin Liu, Jue Wang
                <br>
                <em>2022 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2022, 
                <br>
                <a href="https://mrtornado24.github.io/FENeRF/">[Project]</a>
                <a href="https://arxiv.org/pdf/2111.15490.pdf">[PDF]</a>
                <a href="https://github.com/MrTornado24/FENeRF">[Code]</a>
				<a href="images/fenerf.txt">[BibTeX]</a>
                <br>
                <p>we propose FENeRF, a 3D-aware generator that can produce view-consistent and locally-editable portrait images. Our method uses two decoupled latent codes to generate corresponding facial semantics and texture in a spatial aligned 3D volume with shared geometry. We also reveal that joint learning semantics and texture helps to generate finer geometry.</p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/ijcv22_imocap.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>iMoCap: Motion Capture from Internet Videos</papertitle>
                <br>
                Junting Dong*, Qing Shuai*, <strong>Jingxiang Sun</strong>, Yuanqing Zhang, Hujun Bao, Xiaowei Zhou (* equal contribution)
                <br>
                <em>2022 International Journal of Computer Vision </em>, IJCV 2022, 
                <br>
                <a href="https://link.springer.com/article/10.1007/s11263-022-01596-7">[PDF]</a>
				<a href="images/imocap.txt">[BibTeX]</a>
                <br>
                <p>We propose a novel optimization-based framework and experimentally demonstrate its ability to recover much more precise and detailed motion from multiple videos, compared against monocular pose estimation methods.</p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
                    <img src='images/icbda_bus.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>BusTime: Which is the Right Prediction Model for My Bus Arrival Time?</papertitle>
                <br>
                Dairui Liu, <strong>Jingxiang Sun</strong>, Shen Wang
                <br>
                <em>2020 IEEE International Conference on Big Data Analytics</em>, ICBDA 2020, 
                <br>
                <a href="https://arxiv.org/pdf/2003.10373.pdf">[PDF]</a>
				        <a href="images/bus.txt">[BibTeX]</a>
                <br>
                <p>We proposed a general and practical evaluation framework for analysing various widely used prediction models (i.e. delay, k- nearest-neighbor, kernel regression, additive model, and recur- rent neural network using long short term memory) for bus arrival time.</p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:0%;vertical-align:middle">
            </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
		<hr style="margin-top:0px">
                <p>The website template was adapted from <a href="https://yudeng.github.io/">Yu Deng</a>.</p>
            </td>
        </tr>

      </td>
    </tr>
  </table>
</body>

</html>
